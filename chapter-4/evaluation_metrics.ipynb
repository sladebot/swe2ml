{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# Chapter 4: Evaluation Metrics (The Unit Tests of ML)\n",
    "\n",
    "Accuracy can lie. This notebook shows you why, and introduces the metrics that tell the truth.\n",
    "\n",
    "We'll cover:\n",
    "1. **Confusion Matrix** — The Code Coverage Report\n",
    "2. **Precision & Recall** — Strict Typing vs Test Coverage\n",
    "3. **F1-Score** — The Balanced SLA\n",
    "4. **ROC Curve** — The Load Test\n",
    "5. **Cross-Validation** — The CI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-title",
   "metadata": {},
   "source": [
    "## Setup: Train Our Zoo of Models (from Chapters 2 & 3)\n",
    "\n",
    "Let's start by training the same three models from Chapter 3 on the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the digits dataset (same as Chapter 2)\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train our three models\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions[name] = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, predictions[name])\n",
    "    print(f\"{name}: {acc:.2%} accuracy\")\n",
    "\n",
    "print(\"\\nAll three look great! But accuracy only tells part of the story...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cm-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Confusion Matrix (The Code Coverage Report)\n",
    "\n",
    "A confusion matrix shows you exactly WHERE the model is wrong. It's like a code coverage report — not just \"80% pass rate\" but \"these specific lines are uncovered.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Use the best model (Random Forest)\n",
    "cm = confusion_matrix(y_test, predictions['Random Forest'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=digits.target_names)\n",
    "disp.plot(cmap='Blues', ax=ax, colorbar=True)\n",
    "ax.set_title('Confusion Matrix: Random Forest on Digits', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Diagonal = correct predictions. Off-diagonal = mistakes.\")\n",
    "print(\"Look for the largest off-diagonal numbers — those are the digits the model confuses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cm-compare-title",
   "metadata": {},
   "source": [
    "### Comparing Confusion Matrices Across Models\n",
    "Let's see how each model's errors differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cm-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (name, preds) in zip(axes, predictions.items()):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=digits.target_names)\n",
    "    disp.plot(cmap='Blues', ax=ax, colorbar=False)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    ax.set_title(f'{name}\\nAccuracy: {acc:.1%}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Decision Tree has more off-diagonal noise. Random Forest is cleaner.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-pr-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Precision & Recall (Strict Typing vs Test Coverage)\n",
    "\n",
    "- **Precision**: \"When I predict class X, how often am I right?\" (Strict type checking)\n",
    "- **Recall**: \"Of all actual class X samples, how many did I find?\" (Test coverage)\n",
    "\n",
    "The trade-off is like **latency vs throughput** — you can't maximize both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pr",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "\n",
    "# Detailed report for Random Forest\n",
    "print(\"Random Forest — Classification Report:\")\n",
    "print(\"=\" * 55)\n",
    "print(classification_report(y_test, predictions['Random Forest'], target_names=[str(d) for d in digits.target_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pr-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, predictions['Random Forest'], average=None\n",
    ")\n",
    "\n",
    "x = np.arange(10)\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, precision, width, label='Precision', color='#4ECDC4', edgecolor='white')\n",
    "bars2 = ax.bar(x + width/2, recall, width, label='Recall', color='#FF6B6B', edgecolor='white')\n",
    "\n",
    "ax.set_xlabel('Digit Class', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Precision vs Recall Per Digit (Random Forest)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([str(d) for d in digits.target_names])\n",
    "ax.set_ylim(0.8, 1.05)\n",
    "ax.legend(fontsize=11)\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "worst_precision = digits.target_names[np.argmin(precision)]\n",
    "worst_recall = digits.target_names[np.argmin(recall)]\n",
    "print(f\"Lowest Precision: digit {worst_precision} — model is least confident here\")\n",
    "print(f\"Lowest Recall: digit {worst_recall} — model misses the most of these\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-f1-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. F1-Score (The Balanced SLA)\n",
    "\n",
    "F1 is the harmonic mean of Precision and Recall. Like a balanced SLA — you need BOTH uptime AND response time.\n",
    "\n",
    "### When Accuracy Lies\n",
    "Let's prove it. We'll create an imbalanced dataset and show how accuracy is misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-f1-imbalanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Create a VERY imbalanced dataset: 95% class 0, 5% class 1\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=2000, n_features=20, n_classes=2,\n",
    "    weights=[0.95, 0.05], random_state=42, flip_y=0\n",
    ")\n",
    "\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Class distribution in test set:\")\n",
    "print(f\"  Class 0 (majority): {np.sum(y_test_imb == 0)} samples ({np.mean(y_test_imb == 0):.1%})\")\n",
    "print(f\"  Class 1 (minority): {np.sum(y_test_imb == 1)} samples ({np.mean(y_test_imb == 1):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-f1-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on imbalanced data\n",
    "imb_models = {\n",
    "    'Always Guess\\nMajority Class': DummyClassifier(strategy='most_frequent'),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "imb_results = {}\n",
    "for name, model in imb_models.items():\n",
    "    model.fit(X_train_imb, y_train_imb)\n",
    "    preds = model.predict(X_test_imb)\n",
    "    acc = accuracy_score(y_test_imb, preds)\n",
    "    f1 = f1_score(y_test_imb, preds, average='weighted')\n",
    "    imb_results[name] = {'accuracy': acc, 'f1': f1}\n",
    "    print(f\"{name}: Accuracy={acc:.2%}, F1={f1:.3f}\")\n",
    "\n",
    "print(\"\\nNotice: The 'dumb' model has high accuracy but F1 exposes it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-f1-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = list(imb_results.keys())\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "# Accuracy\n",
    "acc_vals = [imb_results[n]['accuracy'] for n in names]\n",
    "bars = axes[0].bar(range(len(names)), acc_vals, color=colors, edgecolor='white')\n",
    "axes[0].set_xticks(range(len(names)))\n",
    "axes[0].set_xticklabels(names, fontsize=9)\n",
    "axes[0].set_ylim(0.7, 1.05)\n",
    "axes[0].set_title('Accuracy (Misleading!)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Score')\n",
    "for bar, val in zip(bars, acc_vals):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, val + 0.01, f'{val:.1%}',\n",
    "                ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# F1\n",
    "f1_vals = [imb_results[n]['f1'] for n in names]\n",
    "bars = axes[1].bar(range(len(names)), f1_vals, color=colors, edgecolor='white')\n",
    "axes[1].set_xticks(range(len(names)))\n",
    "axes[1].set_xticklabels(names, fontsize=9)\n",
    "axes[1].set_ylim(0.7, 1.05)\n",
    "axes[1].set_title('F1-Score (The Truth)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Score')\n",
    "for bar, val in zip(bars, f1_vals):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, val + 0.01, f'{val:.3f}',\n",
    "                ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Imbalanced Data: Accuracy Lies, F1 Tells the Truth',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-roc-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. The ROC Curve (The Load Test)\n",
    "\n",
    "The ROC curve shows performance at EVERY possible decision threshold—like a load test that measures your API at 10, 100, 1000, and 10000 requests/sec.\n",
    "\n",
    "**AUC** (Area Under the Curve): 1.0 = perfect, 0.5 = random coin flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-roc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# For ROC we need binary classification — use the imbalanced dataset\n",
    "roc_models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3)\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "colors = ['#4ECDC4', '#45B7D1', '#FF6B6B']\n",
    "\n",
    "for (name, model), color in zip(roc_models.items(), colors):\n",
    "    model.fit(X_train_imb, y_train_imb)\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_scores = model.predict_proba(X_test_imb)[:, 1]\n",
    "    else:\n",
    "        y_scores = model.predict(X_test_imb)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test_imb, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, color=color, linewidth=2.5,\n",
    "            label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Random baseline\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random (AUC = 0.500)')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves: Performance Across All Thresholds', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.set_xlim([-0.02, 1.02])\n",
    "ax.set_ylim([-0.02, 1.02])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Higher curve = better model. The dashed line = random guessing.\")\n",
    "print(\"AUC closer to 1.0 means the model separates classes well at ANY threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cv-title",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cross-Validation (The CI Pipeline)\n",
    "\n",
    "A single train/test split = testing on your laptop. Cross-validation = running CI on 5 different machines.\n",
    "\n",
    "**K-Fold Cross-Validation**: Split data into K parts. Train on K-1, test on the remaining 1. Repeat K times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cv",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Run 5-fold cross-validation on the digits dataset\n",
    "cv_models = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "for name, model in cv_models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name}: {scores.mean():.3f} ± {scores.std():.3f}  (scores: {[f'{s:.3f}' for s in scores]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cv-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bp = ax.boxplot(\n",
    "    [cv_results[name] for name in cv_results],\n",
    "    labels=list(cv_results.keys()),\n",
    "    patch_artist=True,\n",
    "    widths=0.5\n",
    ")\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "# Overlay individual fold scores\n",
    "for i, (name, scores) in enumerate(cv_results.items()):\n",
    "    x = np.random.normal(i + 1, 0.04, size=len(scores))\n",
    "    ax.scatter(x, scores, alpha=0.8, color='black', s=40, zorder=3)\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('5-Fold Cross-Validation: Consistency Across Data Splits', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0.85, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Tighter boxplot = more consistent model.\")\n",
    "print(\"Cross-validation gives a more reliable estimate than a single train/test split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Your Metric Cheat Sheet\n",
    "\n",
    "| Metric | SWE Analogy | When to Use |\n",
    "|--------|-------------|-------------|\n",
    "| **Accuracy** | Pass/Fail rate | Balanced classes, quick sanity check |\n",
    "| **Precision** | Strict typing | Cost of false positives is high (spam filter) |\n",
    "| **Recall** | Test coverage | Cost of false negatives is high (disease detection) |\n",
    "| **F1-Score** | Balanced SLA | Imbalanced classes, need both P and R |\n",
    "| **ROC-AUC** | Load test | Comparing models, tuning thresholds |\n",
    "| **Cross-Validation** | CI pipeline | Reliable performance estimate |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-challenge",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge\n",
    "\n",
    "1. Look at the confusion matrix for the **Decision Tree**. Which pair of digits does it confuse the most?\n",
    "2. Change the imbalanced dataset from 95/5 to 99/1 split. How does this affect the \"dumb\" model's accuracy vs F1?\n",
    "3. Try `cross_val_score` with `scoring='f1_weighted'` instead of `'accuracy'`. Do the rankings change?\n",
    "\n",
    "**Next Chapter**: We've been using raw data and default settings. Next, we'll explore **Data Preprocessing & Feature Engineering** — the data pipeline of ML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
